from __future__ import absolute_import, division, print_function, unicode_literals

try:
    # %tensorflow_version only exists in Colab.
    %tensorflow_version 2.x
except Exception:
    pass

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import io
from PIL import Image

from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler, ModelCheckpoint, CSVLogger, ReduceLROnPlateau

import os
import matplotlib.pylab as plt
import numpy as np
import math
import datetime
import pandas as pd

print("Version: ", tf.__version__)
tf.get_logger().setLevel('INFO')

splits, info = tfds.load("horses_or_humans", as_supervised=True, with_info=True,
split=["train[:80%]", "train[80%:]", "test"])

(train_examples, validation_examples, test_examples) = splits

num_examples = info.splits["train"].num_examples
num_classes = info.features["label"].num_classes

SIZE = 150
IMAGE_SIZE = (SIZE, SIZE)

def format_image(image, label):
    image = tf.image.resize(image, IMAGE_SIZE) / 255.0
    return image, label

BATCH_SIZE = 32

train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)
validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)
test_batches = test_examples.map(format_image).batch(1)


def build_model(dense_units, input_shape=IMAGE_SIZE + (3,)):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(16, (3, 3), activation="relu", input_shape=input_shape),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(32, (3, 3), activation="relu"),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(64, (3, 3), activation="relu"),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(2, 2),
        tf.keras.layers.Dense(dense_units, activation="relu"),
        tf.keras.layers.Dense(2, activation="softmax")
    ])
    return model

model = build_model(dense_units=256)
model.compile(
    optimizer="sgd",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# Tensorboard callback
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-$H$M$S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)

model.fit(train_batches,
        epochs=10,
        validation_data=validation_batches,
        callbacks=[tensorboard_callback])

# Model Checkpoint

model = build_model(dense_units=256)
model.compile(
    optimizer="sgd",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(train_batches,
        epochs=1,
        validation_data=validation_batches,
        verbose=2,
        callbacks=[ModelCheckpoint("weights.{epoch:02d}-{val_loss:.2f}.h5", verbose=1)])

model.fit(train_batches,
        epochs=1,
        validation_data=validation_batches,
        verbose=2,
        callbacks=[ModelCheckpoint("saved_model", verbose=1)])

# Early Stopping
model = build_model(dense_units=256)
model.compile(
    optimizer="sgd",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(train_batches,
        epochs=50,
        validation_data=validation_batches,
        verbose=2,
        callbacks=[EarlyStopping(
            patience=3,
            min_delta=0.05,
            baseline=0.8,
            mode="min",
            monitor="val_loss",
            restore_best_weights=True,
            verbose=1
        )])

# CSV logger
model = build_model(dense_units=256)
model.compile(
    optimizer="sgd",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

csv_file = "training.csv"

model.fit(train_batches,
        epochs=5,
        validation_data=validation_batches,
        callbacks=[CSVLogger(csv_file)])

# Learning rate scheduler
model = build_model(dense_units=256)
model.compile(
    optimizer="sgd",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

def step_decay(epoch):
    initial_lr = 0.01
    drop = 0.5
    epochs_drop = 1
    lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lr

model.fit(train_batches,
        epochs=5,
        validation_data=validation_batches,
        callbacks=[LearningRateScheduler(step_decay, verbose=1),
                    TensorBoard(log_dir="./log_dir")])

# ReduceLROnPlateau

model = build_model(dense_units=256)
model.compile(
    optimizer="sgd",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(train_batches,
            epochs=50,
            validation_data=validation_batches,
            callbacks=[ReduceLROnPlateau(monitor="val_loss",
                                        factor=0.2, verbose=1,
                                        patience=1, min_lr=0.001),
                        TensorBoard(log_dir="./log_dir")])
if __name__ == "__main__":
    pass

